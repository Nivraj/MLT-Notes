https://heartbeat.fritz.ai/machine-learning/home

https://en.wikipedia.org/wiki/Gradient_descent
Comes in three flavours -- Simple gradient Descent  
                        -- Stichastic Gradient Descent
                        -- Mini Batch Gradient Descent
                        
Errors are three types:
1. Bais error - This is generated by Faulty assumptions made by Data scientist at the time of selecting Algorithm.
--Most of the time Bias error is Underfiting


2. Variance error -  Difference between the updated prediction and old predicited value is Variance Error.
all Low bias Algorithm has more variance.
High variance will lead to overFiting

3. Irreducebale error - Due to the error in Dataset itself ( Many time happens so error may be found in original data itself)
Example : Sensor's fluctuation which will increase the voltage in data



Loss function 
First order condition(derivative) of the loss  fuction should be zero
1. Sum of Errors
2. Royle's theorem : 
  It has to be continues
  Its RHL and LHL should be available 
  and RHL=LHL should be equal
  
  This absolute fuction is not differentiable. 
3. Sum of squares of Errors:


OLS - Ordinary least square. solution is a closed form solution
Derivate of loss function




Need to see: 
https://blog.algorithmia.com/introduction-to-loss-functions/
https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0+

  -loss function in machine learning
  -Bias, Variance, Trade off -- https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229
  
Words & Terminology-
Accuracy and precision
Bull's Eye view

